{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import requests\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counties_df = pd.read_excel('list-counties-us.xlsx')\n",
    "counties_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_abbr = pd.read_excel('states abbrev.xlsx')\n",
    "states_abbr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counties_df, states_abbr, left_on='State or district', right_on='State', how='left')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# think about the BEST way to prime this data\n",
    "# is it strip county and append the word 'county?'\n",
    "# do you want to filter out anything that does not contain county?\n",
    "\n",
    "merged_df['search_term'] = merged_df['County or equivalent'].str.replace(' ', '-').str.lower()\n",
    "merged_df.head()\n",
    "merged_df['search_term'] = merged_df['search_term'] +  '-' + merged_df['Abbreviation'].str.lower()\n",
    "merged_df['search_term'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = list(merged_df['search_term'])\n",
    "search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped  \n",
    "reviews_list = []\n",
    "\n",
    "#list of counties\n",
    "counties = search_terms [:3] #[ 'autauga-county-al', 'baldwin-county-al']\n",
    "print(counties)\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for county in counties:\n",
    "\n",
    "    \n",
    "#     url = f'https://www.niche.com/places-to-live/search/best-places-to-live/c/{county}/'\n",
    "#     print(url)\n",
    "    \n",
    "   \n",
    "#     #------splinter\n",
    "#     browser.visit(url)\n",
    "#     time.sleep(3) # Sleep for 3 seconds\n",
    "#     html = browser.html\n",
    "#     #Create BeautifulSoup object; parse with 'html.parser'\n",
    "#     review_soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "#     #########################\n",
    "#     # workaround for the human verification stuff\n",
    "#     try:\n",
    "#         human_verify = review_soup.find('div', class_=\"page-title\").find('h1').text\n",
    "#         while human_verify == 'Please verify you are a human':\n",
    "\n",
    "#             time.sleep(30)\n",
    "\n",
    "#             review_soup = BeautifulSoup(html, 'html.parser')\n",
    "#             human_verify = review_soup.find('div', class_=\"page-title\").find('h1').text\n",
    "#     except:\n",
    "#         pass\n",
    "#     #########################\n",
    "    \n",
    "#     #BeautifulSoup scrape\n",
    "#     #    Postgres on AWS\n",
    "#     try:\n",
    "#         comment = ''\n",
    "#         date   = ''\n",
    "#         town  = ''\n",
    "#         county_long  = county \n",
    "#         state = county.split(\"-\")[2].upper()\n",
    "#         county = county.split('-')[0].capitalize()\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "             \n",
    "#                 # Error handling\n",
    "            \n",
    "#     try:\n",
    "#         comment_container = review_soup.find('p', class_=\"search-result-feature\")\n",
    "#         # Identify and return comments, date, place, county of reviews\n",
    "#         comment = comment_container.find('span').text\n",
    "        \n",
    "#        # date = review_soup.find('li', class_=\"review-tagline__item\").text\n",
    "        \n",
    "#         town = review_soup.find('h2', class_=\"search-result__title\").text\n",
    "        \n",
    "#        # county = review_soup.find('div', class_=\"global-nav__input-wrap__sherlock-overlay\").text\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "        \n",
    "#     review_dict = {\n",
    "#         'comment': comment,\n",
    "#         'date': date,\n",
    "#         'town': town,\n",
    "#        # 'county': county,\n",
    "#         'state': state,\n",
    "#         'county_long': county_long\n",
    "#     }\n",
    "        \n",
    "#     reviews_list.append(review_dict)\n",
    "    \n",
    "\n",
    "# reviews_list    \n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_list = []\n",
    "for county in counties:\n",
    "\n",
    "    \n",
    "    url = f'https://www.niche.com/places-to-live/search/best-places-to-live/c/{county}/'\n",
    "    print(url)\n",
    "   \n",
    "   \n",
    "    #------splinter\n",
    "    browser.visit(url)    \n",
    "    time.sleep(3) # Sleep for 3 seconds\n",
    "    html = browser.html\n",
    "    #Create BeautifulSoup object; parse with 'html.parser'\n",
    "    review_soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    #########################\n",
    "    # workaround for the human verification stuff\n",
    "    try:\n",
    "        human_verify = review_soup.find('div', class_=\"page-title\").find('h1').text\n",
    "        while human_verify == 'Please verify you are a human':\n",
    "\n",
    "            time.sleep(30)\n",
    "\n",
    "            review_soup = BeautifulSoup(html, 'html.parser')\n",
    "            human_verify = review_soup.find('div', class_=\"page-title\").find('h1').text\n",
    "    except:\n",
    "        pass\n",
    "    #########################\n",
    "    \n",
    "    #BeautifulSoup scrape\n",
    "    #    Postgres on AWS\n",
    "    county_long  = county \n",
    "    state = county.split(\"-\")[2].upper()\n",
    "    county = county.split('-')[0].capitalize()\n",
    "    commentssoup = []\n",
    "             \n",
    "                # Error handling\n",
    "            \n",
    "    try:\n",
    "        comment_containers = review_soup.find_all('p', class_=\"search-result-feature\")\n",
    "        print(len(comment_containers))\n",
    "        for container in comment_containers:\n",
    "            print(container)\n",
    "            comment = ''\n",
    "            date   = ''\n",
    "            town  = ''\n",
    "\n",
    "           \n",
    "            # Identify and return comments, date, place, county of reviews\n",
    "            comment = container.find('span').text\n",
    "            #reviews.append(comment)\n",
    "        \n",
    "            # date = review_soup.find('li', class_=\"review-tagline__item\").text\n",
    "        \n",
    "            town = review_soup.find('h2', class_=\"search-result__title\").text\n",
    "        \n",
    "            # county = review_soup.find('div', class_=\"global-nav__input-wrap__sherlock-overlay\").text\n",
    "            \n",
    "            review_dict = {\n",
    "            'comment': comment,\n",
    "            #'date': date,\n",
    "            'town': town,\n",
    "            # 'county': county,\n",
    "            'state': state,\n",
    "            'county_long': county_long\n",
    "            }\n",
    "\n",
    "            reviews_list.append(review_dict)\n",
    "            print(reviews_list)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "reviews_list    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_list\n",
    "reviews_df = pd.DataFrame(reviews_list)\n",
    "reviews_df.head(20)\n",
    "len(reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import remote_db_endpoint, remote_db_port\n",
    "from config import remote_db_name, remote_db_user, remote_db_pwd\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f\"mysql://{remote_db_user}:{remote_db_pwd}@{remote_db_endpoint}:{remote_db_port}/{remote_db_name}\")\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.to_sql(name='review_webscraping', con=engine, if_exists = 'replace', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
